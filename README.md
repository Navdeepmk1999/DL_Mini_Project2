# DL_Mini_Project2

# Parameterâ€‘Efficient RoBERTa Fineâ€‘Tuning with LoRA on AGÂ News

*Satya Deep Dasari* (gd2576@nyu.edu)  
*Navdeep Mugathihalli Kumaregowda* (nm4686@nyu.edu)  
NewÂ York University â€” CSCIâ€‘GA Deep Learning SpringÂ 2025

ðŸ”— *GitHub:* https://github.com/Navdeepmk1999/DL_Mini_Project2  
ðŸ“„ *Report:* [DLProject2 (3).pdf](./DLProject2%20(3).pdf)  
ðŸ““ *Notebook:* [dl-p2.ipynb](./dl-p2.ipynb)

---

## ðŸš€ Project Goal

- Fineâ€‘tune a *frozen RoBERTaâ€‘base* model on *AGÂ News* by training only *LoRA adapters*  
- Limit *trainable parameters* to *<Â 1Â million* (888Â 580 total)  
- Achieve *â‰¥Â 84.6Â %* test accuracyâ€”within 0.4Â pp of full fineâ€‘tuning  
- Shrink checkpoint size from 476Â MB â†’ 3Â MB for easy sharing and onâ€‘device use  

---

## ðŸ—‚ï¸ Dataset

- *AGÂ News* (Hugging Face):  
  - 120Â 000 train articles, 7Â 600 test articles  
  - 4 balanced classes: World, Sports, Business, Sci/Tech  
- Tokenization via roberta-base tokenizer (maxÂ 512 tokens)  

---

## ðŸ—ï¸ Model & LoRA Setup

| Component            | Configuration                  |
|----------------------|--------------------------------|
| *Backbone*         | roberta-base, 12 layers, 125Â 537Â 288 params (frozen) |
| *LoRA rank (r)*    | 8                              |
| *LoRA Î±*           | 16                             |
| *LoRA dropout*     | 0.1                            |
| *Target modules*   | query, value projections   |
| *Trainable params* | 880Â 896Â (adapters) +Â 7Â 684Â (head) =Â 888Â 580 (0.7Â %) |
| *Checkpoint size*  | â‰ˆÂ 3Â MB                          |

---

## ðŸ› ï¸ Training Configuration

- *Epochs:*Â 3  
- *Optimizer:*Â AdamW (lrÂ =Â 5Â Ã—Â 10â»âµ, weightÂ decayÂ =Â 0.01)  
- *Scheduler:*Â 10Â % linear warmâ€‘up â†’ cosine decay  
- *Loss:*Â Crossâ€‘entropy + label smoothing (ÎµÂ =Â 0.1)  
- *Batching:*Â physicalÂ 16, gradâ€‘accumÂ 2 (eff.Â 32)  
- *Precision:*Â FP16 (PyTorchÂ AMP)  
- *Early stopping:*Â best validation checkpoint retained  

---

## ðŸ“ˆ Results

| Metric                 | Value            |
|------------------------|------------------|
| *Test accuracy*      | 84.6Â %           |
| *Trainable parameters* | 888Â 580         |
| *Total parameters*   | 125Â 537Â 288      |
| *Checkpoint size*    | â‰ˆÂ 3Â MB           |
| *Training time*      | â‰ˆÂ 25Â min (RTXâ€‘3060) |

![Accuracy per Epoch](./figures/accuracy.png)  
Figure: Validation accuracy rises from 90.8Â % â†’ 92.3Â % over 4 epochs.

![Loss per Epoch](./figures/loss.png)  
Figure: Training loss drops from 1.39 â†’Â 0.20, evaluation loss from 0.27Â â†’Â 0.22.

![Confusion Matrix](./figures/conf_mat.png)  
Figure: Most confusion between Business â†”Â Sci/Tech; overall recall 90â€“97Â %.

![Classification Report](./figures/classification_report.png)  
Figure: Precision/recall/F1 by class; macroâ€‘avg F1Â =Â 0.92.

---

## ðŸ“¦ Usage

1. *Clone & install*  
   ```bash
   git clone https://github.com/Navdeepmk1999/DL_Mini_Project2.git
   cd DL_Mini_Project2
   pip install -r requirements.txt
